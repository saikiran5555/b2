{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1b5288",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners, typically decision trees, through a sequential process where each tree is trained to correct the errors made by the ensemble of the previously trained trees. This process involves iteratively fitting new models to provide a more accurate estimate of the response variable. Here's a step-by-step explanation of how Gradient Boosting builds an ensemble of weak learners:\n",
    "\n",
    "1. Initialize with a Base Model\n",
    "Gradient Boosting starts with an initial prediction. This can be as simple as the mean of the target values for regression problems or the log odds for classification problems. This initial model serves as the starting point for the iterative process.\n",
    "\n",
    "2. Sequentially Add Weak Learners\n",
    "The core of the Gradient Boosting method involves adding weak learners one at a time. After the initial model, the algorithm enters a loop where it repeatedly adds new weak learners. Each new weak learner is trained on the residuals of the combined ensemble from the previous step.\n",
    "\n",
    "3. Compute Residuals\n",
    "For each iteration, the algorithm computes the residuals or errors between the observed values and the predictions of the current ensemble. These residuals represent the direction in which we want our next model to improve. In the context of Gradient Boosting, \"gradient\" refers to the gradient of the loss function that is being minimized, and these residuals are a way to quantify the negative gradient.\n",
    "\n",
    "4. Train the Next Weak Learner on Residuals\n",
    "Each new weak learner is trained to predict the residuals from the previous step. By focusing on the residuals, each weak learner aims to correct the mistakes of the ensemble so far. This means that rather than fitting the raw data, each new model is specifically trained to improve areas where the current ensemble performs poorly.\n",
    "\n",
    "5. Update the Ensemble\n",
    "After training a new weak learner, the algorithm updates the ensemble's predictions. This is done by adding the new weak learner's predictions to the ensemble, weighted by a learning rate (also known as the \"shrinkage\" parameter). The learning rate is a small positive number (less than 1) that scales the contribution of each weak learner, helping to prevent overfitting by making the model training process more gradual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
