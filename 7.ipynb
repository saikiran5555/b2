{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e887285a",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the key concepts and steps involved in the algorithm. Here's a breakdown of the steps involved in developing the mathematical intuition behind Gradient Boosting:\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "The process begins with defining a loss function \n",
    "�\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    "^\n",
    ")\n",
    "L(y, \n",
    "y\n",
    "^\n",
    "​\n",
    " ) that measures the discrepancy between the actual target values \n",
    "�\n",
    "y and the predicted values \n",
    "�\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    " . Common loss functions for regression problems include mean squared error (MSE) or mean absolute error (MAE).\n",
    "Initial Model:\n",
    "\n",
    "An initial model is created, often a simple one like the mean value of the target variable. Let's denote this model as \n",
    "�\n",
    "0\n",
    "(\n",
    "�\n",
    ")\n",
    "F \n",
    "0\n",
    "​\n",
    " (x).\n",
    "Residual Calculation:\n",
    "\n",
    "The residuals are calculated by subtracting the predictions of the current model from the actual target values:\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "r \n",
    "i\n",
    "​\n",
    " =y \n",
    "i\n",
    "​\n",
    " −F \n",
    "t−1\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " )\n",
    "where \n",
    "�\n",
    "�\n",
    "r \n",
    "i\n",
    "​\n",
    "  is the residual for the \n",
    "�\n",
    "i-th sample, \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the actual target value, and \n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "F \n",
    "t−1\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ) is the prediction of the current model for the \n",
    "�\n",
    "i-th sample.\n",
    "Weak Learner Fitting:\n",
    "\n",
    "A weak learner (often a decision tree) is fitted to the residuals. The goal is to find a model \n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "h \n",
    "t\n",
    "​\n",
    " (x) that minimizes the loss function with respect to the residuals:\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "argmin\n",
    "ℎ\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "+\n",
    "ℎ\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    ")\n",
    "h \n",
    "t\n",
    "​\n",
    " (x)=argmin \n",
    "h\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "N\n",
    "​\n",
    " L(y \n",
    "i\n",
    "​\n",
    " ,F \n",
    "t−1\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " )+h(x \n",
    "i\n",
    "​\n",
    " ))\n",
    "where \n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "h \n",
    "t\n",
    "​\n",
    " (x) is the weak learner for the \n",
    "�\n",
    "t-th iteration.\n",
    "Learning Rate:\n",
    "\n",
    "A learning rate \n",
    "�\n",
    "η is introduced to control the contribution of each weak learner to the ensemble. The predictions of the weak learner are scaled by the learning rate before being added to the ensemble:\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "(\n",
    "�\n",
    ")\n",
    "+\n",
    "�\n",
    "⋅\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "F \n",
    "t\n",
    "​\n",
    " (x)=F \n",
    "t−1\n",
    "​\n",
    " (x)+η⋅h \n",
    "t\n",
    "​\n",
    " (x)\n",
    "Update Ensemble Model:\n",
    "\n",
    "The predictions of the weak learner are added to the ensemble model, weighted by the learning rate:\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "(\n",
    "�\n",
    ")\n",
    "+\n",
    "�\n",
    "⋅\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "F \n",
    "t\n",
    "​\n",
    " (x)=F \n",
    "t−1\n",
    "​\n",
    " (x)+η⋅h \n",
    "t\n",
    "​\n",
    " (x)\n",
    "Repeat:\n",
    "\n",
    "Steps 3 to 6 are repeated for a specified number of iterations (or until a convergence criterion is met). Each iteration focuses on reducing the remaining residuals and improving the overall model's performance.\n",
    "Final Prediction:\n",
    "\n",
    "The final prediction is obtained by summing the predictions of all weak learners in the ensemble:\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "0\n",
    "(\n",
    "�\n",
    ")\n",
    "+\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "⋅\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "F(x)=F \n",
    "0\n",
    "​\n",
    " (x)+∑ \n",
    "t=1\n",
    "T\n",
    "​\n",
    " η⋅h \n",
    "t\n",
    "​\n",
    " (x)\n",
    "where \n",
    "�\n",
    "T is the total number of iterations.\n",
    "By iteratively fitting weak learners to the residuals of the previous models and updating the ensemble model accordingly, Gradient Boosting gradually improves the overall model's performance, ultimately leading to a strong predictive model for regression problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
